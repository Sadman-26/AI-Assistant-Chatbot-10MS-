# ================================================================
# COMPLETE BANGLA EDUCATIONAL RAG SYSTEM
# World-Class AI Engineering Implementation
# ================================================================

print("üöÄ Initializing Bangla Educational RAG System...")
print("=" * 60)

# ================================================================
# STEP 1: Import Required Libraries
# ================================================================
print("\nüìö Importing required libraries...")

import os
import sys
import warnings
import glob
import json
from pathlib import Path
from datetime import datetime
warnings.filterwarnings('ignore')
from dotenv import load_dotenv
# Core libraries
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from sentence_transformers import SentenceTransformer

# Text processing
import re
import unicodedata
from collections import defaultdict

# LangChain components
from langchain.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.embeddings.base import Embeddings
from langchain_groq import ChatGroq
from langchain.schema import Document

# Vector database
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore

print("‚úÖ All libraries imported successfully!")

# ================================================================
# STEP 2: Configuration and Constants
# ================================================================
print("\n‚öôÔ∏è Setting up configuration...")

class Config:
    """Configuration class for the RAG system"""
    
    # Paths
    DOC_FOLDER = "doc"  # Your document folder
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    
    # Model settings
    EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
    LLM_MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"  # Better model for Bangla
    TEMPERATURE = 0.1
    MAX_TOKENS = 2048
    
    # Retrieval settings
    TOP_K_DOCS = 5
    
    # Pinecone settings
    INDEX_NAME = "rag"
    DIMENSION = 1024  #embedder model dimension
    METRIC = "cosine"
    
    # Language settings
    PRIMARY_LANGUAGE = "bangla"
    SECONDARY_LANGUAGE = "english"

config = Config()

# ================================================================
# STEP 3: API Keys Setup
# ================================================================
print("\nüîë Setting up API keys...")

load_dotenv()

# Get API key from environment variable
groq_api_key = os.getenv("GROQ_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")

# Set environment variables

print("‚úÖ API keys configured!")

# ================================================================
# STEP 4: Bangla Text Processing Utilities
# ================================================================
print("\nüî§ Setting up Bangla text processing utilities...")

class BanglaTextProcessor:
    """Advanced Bangla text processing utilities"""
    
    @staticmethod
    def clean_bangla_text(text: str) -> str:
        """Clean and normalize Bangla text"""
        if not text:
            return ""
        
        # Normalize Unicode
        text = unicodedata.normalize('NFC', text)
        
        # Remove extra whitespaces
        text = re.sub(r'\s+', ' ', text)
        
        # Remove unwanted characters but keep Bangla punctuation
        text = re.sub(r'[^\u0980-\u09FF\u0020-\u007E\s\.\,\!\?\;\:\'\"\(\)\-]', '', text)
        
        # Clean up punctuation spacing
        text = re.sub(r'\s*([‡•§,!?;:])\s*', r'\1 ', text)
        
        return text.strip()
    
    @staticmethod
    def extract_metadata_from_filename(filename: str) -> Dict[str, str]:
        """Extract metadata from filename patterns"""
        filename = Path(filename).stem
        
        # Common patterns for educational files (optional)
        patterns = {
            'subject': r'(‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ|‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø|‡¶ó‡¶£‡¶ø‡¶§|‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®|‡¶∏‡¶Æ‡¶æ‡¶ú|‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏|‡¶≠‡ßÇ‡¶ó‡ßã‡¶≤)',
            'class': r'(class|‡¶∂‡ßç‡¶∞‡ßá‡¶£‡¶ø)[\s\-_]*(\d+|‡¶è‡¶ï|‡¶¶‡ßÅ‡¶á|‡¶§‡¶ø‡¶®|‡¶ö‡¶æ‡¶∞|‡¶™‡¶æ‡¶Å‡¶ö|‡¶õ‡¶Ø‡¶º|‡¶∏‡¶æ‡¶§|‡¶Ü‡¶ü|‡¶®‡¶Ø‡¶º|‡¶¶‡¶∂)',
            'chapter': r'(chapter|‡¶Ö‡¶ß‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º)[\s\-_]*(\d+)',
            'book': r'(book|‡¶¨‡¶á)[\s\-_]*(.+)',
        }
        
        metadata = {}
        for key, pattern in patterns.items():
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                if key in ['class', 'chapter']:
                    metadata[key] = match.group(2)
                else:
                    metadata[key] = match.group(1)
        
        metadata['filename'] = filename
        return metadata
    
    @staticmethod
    def detect_content_type(text: str) -> str:
        """Detect the type of educational content"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['‡¶ï‡¶¨‡¶ø‡¶§‡¶æ', 'poem', '‡¶õ‡¶®‡ßç‡¶¶', '‡¶∞‡¶æ‡¶á‡¶Æ']):
            return 'poetry'
        elif any(word in text_lower for word in ['‡¶ó‡¶≤‡ßç‡¶™', 'story', '‡¶â‡¶™‡¶®‡ßç‡¶Ø‡¶æ‡¶∏', 'novel']):
            return 'literature'
        elif any(word in text_lower for word in ['‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶∞‡¶£', 'grammar', '‡¶¨‡¶æ‡¶®‡¶æ‡¶®', 'spelling']):
            return 'grammar'
        elif any(word in text_lower for word in ['‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏', 'history', 'historical']):
            return 'history'
        elif any(word in text_lower for word in ['‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®', 'science', 'scientific']):
            return 'science'
        elif any(word in text_lower for word in ['‡¶ó‡¶£‡¶ø‡¶§', 'math', 'mathematics']):
            return 'mathematics'
        else:
            return 'general'

text_processor = BanglaTextProcessor()

# ================================================================
# STEP 5: Enhanced Multilingual Embeddings
# ================================================================
print("\nüß† Setting up Enhanced Multilingual Embeddings...")

class EnhancedBanglaEmbeddings(Embeddings):
    """
    Enhanced embedding class optimized for Bangla educational content
    """
    
    def __init__(self, model_name: str = config.EMBEDDING_MODEL):
        print(f"Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)
        
        # Optimize for multilingual performance
        self.model.max_seq_length = 512
        
        print("‚úÖ Enhanced Bangla embedding model loaded successfully!")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed documents with optimized prefixes for educational content"""
        # Clean texts first
        cleaned_texts = [text_processor.clean_bangla_text(text) for text in texts]
        
        # Add educational passage prefix for better retrieval
        prefixed_texts = [f"passage: {text}" for text in cleaned_texts]
        
        # Generate embeddings with normalization
        embeddings = self.model.encode(
            prefixed_texts, 
            normalize_embeddings=True,
            batch_size=32,
            show_progress_bar=True
        )
        return embeddings.tolist()
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query with educational context"""
        # Clean query text
        cleaned_text = text_processor.clean_bangla_text(text)
        
        # Add educational query prefix
        prefixed_text = f"query: {cleaned_text}"
        
        # Generate embedding
        embedding = self.model.encode(
            prefixed_text, 
            normalize_embeddings=True
        )
        return embedding.tolist()

# Initialize embeddings
embeddings = EnhancedBanglaEmbeddings()

# ================================================================
# STEP 6: Document Loader and Processor
# ================================================================
print("\nüìÑ Setting up document loader and processor...")

class BanglaDocumentLoader:
    """Advanced document loader for Bangla educational content"""
    
    def __init__(self, doc_folder: str = config.DOC_FOLDER):
        self.doc_folder = doc_folder
        self.supported_formats = ['.txt', '.md']
    
    def load_documents(self) -> List[Document]:
        """Load all documents from the doc folder with enhanced metadata"""
        documents = []
        
        if not os.path.exists(self.doc_folder):
            print(f"‚ö†Ô∏è Creating doc folder: {self.doc_folder}")
            os.makedirs(self.doc_folder)
            print("üìù Please add your Bangla educational txt files to the 'doc' folder")
            return documents
        
        # Get all text files
        file_patterns = [f"**/*{ext}" for ext in self.supported_formats]
        all_files = []
        
        for pattern in file_patterns:
            all_files.extend(glob.glob(os.path.join(self.doc_folder, pattern), recursive=True))
        
        print(f"üìö Found {len(all_files)} files to process")
        
        for file_path in all_files:
            try:
                # Load document with proper encoding
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if not content.strip():
                    continue
                
                # Clean content
                cleaned_content = text_processor.clean_bangla_text(content)
                
                # Extract metadata
                base_metadata = text_processor.extract_metadata_from_filename(file_path)
                
                # Enhanced metadata
                metadata = {
                    **base_metadata,
                    'source': file_path,
                    'file_size': len(content),
                    'content_type': text_processor.detect_content_type(content),
                    'language': 'bangla' if self._is_bangla_content(content) else 'mixed',
                    'processed_at': datetime.now().isoformat(),
                    'char_count': len(cleaned_content),
                    'word_count': len(cleaned_content.split())
                }
                
                # Create document
                doc = Document(
                    page_content=cleaned_content,
                    metadata=metadata
                )
                documents.append(doc)
                
                print(f"‚úÖ Processed: {os.path.basename(file_path)}")
                
            except Exception as e:
                print(f"‚ùå Error processing {file_path}: {str(e)}")
                continue
        
        print(f"‚úÖ Successfully loaded {len(documents)} documents")
        return documents
    
    def _is_bangla_content(self, text: str) -> bool:
        """Check if text is primarily in Bangla"""
        bangla_chars = len(re.findall(r'[\u0980-\u09FF]', text))
        total_chars = len(re.findall(r'[^\s\d\W]', text))
        return bangla_chars > (total_chars * 0.6) if total_chars > 0 else False

# ================================================================
# STEP 7: Advanced Text Splitter
# ================================================================
print("\n‚úÇÔ∏è Setting up advanced text splitter...")

class BanglaTextSplitter(RecursiveCharacterTextSplitter):
    """Custom text splitter optimized for Bangla educational content"""
    
    def __init__(self, chunk_size: int = config.CHUNK_SIZE, chunk_overlap: int = config.CHUNK_OVERLAP):
        # Bangla-specific separators
        separators = [
            "\n\n",  # Paragraph breaks
            "‡•§\n",   # Bangla sentence end
            "‡•§",     # Bangla sentence end
            "\n",    # Line breaks
            "‡•§‡•§",    # Double danda
            "‡•§‡•§‡•§",   # Triple danda
            ". ",    # English sentence end
            "! ",    # Exclamation
            "? ",    # Question
            "; ",    # Semicolon
            ", ",    # Comma
            " ",     # Space
            ""       # Character level
        ]
        
        super().__init__(
            separators=separators,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False
        )

# ================================================================
# STEP 8: Load and Process Documents
# ================================================================
print("\nüìñ Loading and processing documents...")

# Initialize document loader
doc_loader = BanglaDocumentLoader()

# Load documents
documents = doc_loader.load_documents()

if not documents:
    print("‚ö†Ô∏è No documents found. Creating sample documents for demonstration...")
    
    # Create sample educational content
    sample_docs = [
        Document(
            page_content="""
            ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶ì ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø

            ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶¶‡¶ï‡ßç‡¶∑‡¶ø‡¶£ ‡¶è‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶≠‡¶æ‡¶∑‡¶æ‡•§ ‡¶è‡¶ü‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∞‡¶æ‡¶∑‡ßç‡¶ü‡ßç‡¶∞‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó ‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø‡ßá‡¶∞ ‡¶¶‡¶æ‡¶™‡ßç‡¶§‡¶∞‡¶ø‡¶ï ‡¶≠‡¶æ‡¶∑‡¶æ‡•§

            ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø‡ßá‡¶∞ ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏ ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶∏‡¶Æ‡ßÉ‡¶¶‡ßç‡¶ß‡•§ ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞, ‡¶®‡¶ú‡¶∞‡ßÅ‡¶≤ ‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ, ‡¶¨‡¶ô‡ßç‡¶ï‡¶ø‡¶Æ‡¶ö‡¶®‡ßç‡¶¶‡ßç‡¶∞ ‡¶ö‡¶ü‡ßç‡¶ü‡ßã‡¶™‡¶æ‡¶ß‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶Æ‡ßÅ‡¶ñ ‡¶Æ‡¶π‡¶æ‡¶® ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø‡¶ø‡¶ï‡¶¶‡ßá‡¶∞ ‡¶Ö‡¶¨‡¶¶‡¶æ‡¶®‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶Æ‡¶æ‡¶®‡ßá‡¶∞‡•§

            ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶∞‡¶£:
            ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶∞‡¶£‡ßá ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡ßç‡¶Ø, ‡¶∏‡¶∞‡ßç‡¶¨‡¶®‡¶æ‡¶Æ, ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶£, ‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ, ‡¶Ö‡¶¨‡ßç‡¶Ø‡¶Ø‡¶º - ‡¶è‡¶á ‡¶™‡¶æ‡¶Å‡¶ö‡¶ü‡¶ø ‡¶Æ‡ßÇ‡¶≤ ‡¶™‡¶¶ ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§
            """,
            metadata={
                'subject': '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ',
                'content_type': 'literature',
                'language': 'bangla',
                'source': 'sample_bangla_literature.txt'
            }
        ),
        Document(
            page_content="""
            ‡¶õ‡¶®‡ßç‡¶¶ ‡¶ì ‡¶ï‡¶¨‡¶ø‡¶§‡¶æ

            ‡¶õ‡¶®‡ßç‡¶¶ ‡¶π‡¶≤‡ßã ‡¶ï‡¶¨‡¶ø‡¶§‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡¶£‡•§ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶õ‡¶®‡ßç‡¶¶‡ßá‡¶∞ ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶ó:
            ‡ßß. ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞‡¶¨‡ßÉ‡¶§‡ßç‡¶§ ‡¶õ‡¶®‡ßç‡¶¶
            ‡ß®. ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ‡¶¨‡ßÉ‡¶§‡ßç‡¶§ ‡¶õ‡¶®‡ßç‡¶¶  
            ‡ß©. ‡¶∏‡ßç‡¶¨‡¶∞‡¶¨‡ßÉ‡¶§‡ßç‡¶§ ‡¶õ‡¶®‡ßç‡¶¶

            ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶•‡ßá‡¶∞ ‡¶¨‡¶ø‡¶ñ‡ßç‡¶Ø‡¶æ‡¶§ ‡¶ï‡¶¨‡¶ø‡¶§‡¶æ:
            "‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡ßã‡¶®‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ, ‡¶Ü‡¶Æ‡¶ø ‡¶§‡ßã‡¶Æ‡¶æ‡¶Ø‡¶º ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø‡•§
            ‡¶ö‡¶ø‡¶∞‡¶¶‡¶ø‡¶® ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶Ü‡¶ï‡¶æ‡¶∂, ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶§‡¶æ‡¶∏, ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡¶£‡ßá ‡¶¨‡¶æ‡¶ú‡¶æ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶Å‡¶∂‡¶ø‡•§"

            ‡¶è‡¶ü‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶∏‡¶Ç‡¶ó‡ßÄ‡¶§‡•§
            """,
            metadata={
                'subject': '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ',
                'content_type': 'poetry',
                'language': 'bangla',
                'source': 'sample_poetry.txt'
            }
        )
    ]
    documents = sample_docs

# Initialize text splitter
text_splitter = BanglaTextSplitter()

# Split documents into chunks
print(f"üìù Splitting {len(documents)} documents into chunks...")
text_chunks = text_splitter.split_documents(documents)

print(f"‚úÖ Created {len(text_chunks)} text chunks")

# Display sample chunk info
if text_chunks:
    sample_chunk = text_chunks[0]
    print(f"\nüìã Sample chunk preview:")
    print(f"Content length: {len(sample_chunk.page_content)} characters")
    print(f"Metadata: {sample_chunk.metadata}")
    print(f"Content preview: {sample_chunk.page_content[:200]}...")

# ================================================================
# STEP 9: Initialize GROQ LLM
# ================================================================
print("\nü§ñ Setting up Enhanced GROQ LLM...")

# Initialize the GROQ LLM with better model for Bangla
llm = ChatGroq(
    model=config.LLM_MODEL,
    temperature=config.TEMPERATURE,
    max_tokens=config.MAX_TOKENS,
    timeout=60,
    max_retries=3
)

print("‚úÖ Enhanced GROQ LLM initialized successfully!")

# ================================================================
# STEP 10: Setup Pinecone Vector Database
# ================================================================
print("\nüóÑÔ∏è Setting up Enhanced Pinecone Vector Database...")

# Initialize Pinecone client
pc = Pinecone(api_key=pinecone_api_key)

# Check if index exists, create if not
existing_indexes = pc.list_indexes().names()
if config.INDEX_NAME not in existing_indexes:
    print(f"Creating new Pinecone index: {config.INDEX_NAME}")
    pc.create_index(
        name=config.INDEX_NAME,
        dimension=config.DIMENSION,
        metric=config.METRIC,
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )
    print("‚úÖ New Pinecone index created!")
else:
    print(f"‚úÖ Using existing Pinecone index: {config.INDEX_NAME}")

# Get index reference
index = pc.Index(config.INDEX_NAME)

# ================================================================
# STEP 11: Create Vector Store and Retriever
# ================================================================
print("\nüîç Creating enhanced vector store and retriever...")

# Check current vectors in index
stats = index.describe_index_stats()
total_vectors = stats.get('total_vector_count', 0)
print(f"Current vectors in index: {total_vectors}")

# Create or connect to vectorstore
vectorstore = PineconeVectorStore.from_documents(
    documents=text_chunks,
    index_name=config.INDEX_NAME,
    embedding=embeddings
) if total_vectors == 0 else PineconeVectorStore(
    index_name=config.INDEX_NAME,
    embedding=embeddings
)

# Setup enhanced retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": config.TOP_K_DOCS}
)

print("‚úÖ Enhanced vector store and retriever created!")

# ================================================================
# STEP 12: Bangla Educational Prompt Template
# ================================================================
print("\nüìù Setting up Bangla educational prompt template...")

# Enhanced system prompt for Bangla education
system_prompt = """
‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û ‡¶è‡¶¨‡¶Ç ‡¶¶‡¶ï‡ßç‡¶∑ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ï‡•§ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶¶‡¶æ‡¶Ø‡¶º‡¶ø‡¶§‡ßç‡¶¨ ‡¶π‡¶≤‡ßã ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶∞‡ßç‡¶•‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ, ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶∏‡¶†‡¶ø‡¶ï ‡¶ì ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï ‡¶§‡¶•‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ‡•§

## ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶§‡ßç‡¶¨:
- ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶ì ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶∞‡¶£
- ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø ‡¶ì ‡¶ï‡¶¨‡¶ø‡¶§‡¶æ
- ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø ‡¶ì ‡¶ê‡¶§‡¶ø‡¶π‡ßç‡¶Ø
- ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶æ‡¶¨‡¶≤‡ßÄ

## ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶®‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:

### ‡ßß. ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶ì ‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤:
- ‡¶∏‡¶∞‡ßç‡¶¨‡¶¶‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®
- ‡¶∏‡¶π‡¶ú, ‡¶∏‡¶∞‡¶≤ ‡¶ì ‡¶¨‡ßã‡¶ß‡¶ó‡¶Æ‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶∞‡ßç‡¶•‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶â‡¶™‡¶Ø‡ßã‡¶ó‡ßÄ ‡¶ï‡¶∞‡ßá ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®‡ßá ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶¶‡¶ø‡¶®

### ‡ß®. ‡¶§‡¶•‡ßç‡¶Ø‡ßá‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø‡¶§‡¶æ:
- ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ context ‡¶•‡ßá‡¶ï‡ßá ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶®‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ
- ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶Ø‡¶ñ‡¶® ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®

### ‡ß©. ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø:
- ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶Æ‡ßÇ‡¶≤ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶¨‡¶≤‡ßÅ‡¶®
- ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¶‡¶ø‡¶®
- ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶∏‡¶π ‡¶â‡¶™‡¶∏‡ßç‡¶•‡¶æ‡¶™‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®

### ‡ß™. ‡¶â‡¶§‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶ï‡¶æ‡¶†‡¶æ‡¶Æ‡ßã:
- ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶ì ‡¶∏‡¶Ç‡¶ó‡¶†‡¶ø‡¶§ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®
- ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®‡ßá ‡¶™‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü ‡¶Ü‡¶ï‡¶æ‡¶∞‡ßá ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®
- ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º ‡¶§‡ßÅ‡¶≤‡ßá ‡¶ß‡¶∞‡ßÅ‡¶®

### ‡ß´. ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá:
- ‡¶ï‡¶¨‡¶ø‡¶§‡¶æ ‡¶¨‡¶æ ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø‡ßá‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶≠‡¶æ‡¶¨ ‡¶ì ‡¶Ö‡¶∞‡ßç‡¶• ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶∞‡¶£‡ßá‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ ‡¶ì ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶¶‡¶ø‡¶®
- ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏ ‡¶¨‡¶æ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶™‡ßç‡¶∞‡ßá‡¶ï‡ßç‡¶∑‡¶æ‡¶™‡¶ü ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßÅ‡¶®

### ‡ß¨. ‡¶Ö‡¶ú‡¶æ‡¶®‡¶æ ‡¶§‡¶•‡ßç‡¶Ø‡ßá‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá:
‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ ‡¶§‡¶•‡ßç‡¶Ø‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶¨‡¶≤‡ßÅ‡¶®:
"‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§ ‡¶§‡¶•‡ßç‡¶Ø‡ßá ‡¶è‡¶á ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡ßá ‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü ‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á‡•§ ‡¶Ü‡¶∞‡¶ì ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶¨‡¶æ ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û‡ßá‡¶∞ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶®‡¶ø‡¶®‡•§"

‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó (Context): {context}

‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á:
‚úì ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
‚úì ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá  
‚úì ‡¶∏‡¶†‡¶ø‡¶ï ‡¶ì ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
‚úì ‡¶∏‡¶π‡¶ú‡¶¨‡ßã‡¶ß‡ßç‡¶Ø ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
"""

# Create enhanced prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

print("‚úÖ Bangla educational prompt template configured!")

# ================================================================
# STEP 13: Create Enhanced RAG Chain
# ================================================================
print("\nüîó Creating Enhanced RAG Chain...")

# Create document chain
question_answer_chain = create_stuff_documents_chain(llm, prompt)

# Create retrieval chain
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

print("‚úÖ Enhanced RAG chain created successfully!")

# ================================================================
# STEP 14: Advanced Query System
# ================================================================
print("\nüéØ Setting up Advanced Query System...")

class BanglaRAG:
    """Advanced Bangla Educational RAG System"""
    
    def __init__(self, rag_chain, retriever):
        self.rag_chain = rag_chain
        self.retriever = retriever
        self.query_history = []
    
    def query(self, question: str, show_sources: bool = True) -> Dict[str, Any]:
        """
        Process educational query with enhanced features
        """
        print(f"\nüîç ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶ï‡¶∞‡¶£: {question}")
        
        # Clean and process question
        cleaned_question = text_processor.clean_bangla_text(question)
        
        # Get response from RAG chain
        try:
            response = self.rag_chain.invoke({"input": cleaned_question})
            
            # Extract information
            answer = response.get("answer", "")
            source_docs = response.get("context", [])
            
            # Prepare result
            result = {
                "question": question,
                "answer": answer,
                "source_documents": source_docs,
                "num_sources": len(source_docs),
                "timestamp": datetime.now().isoformat()
            }
            
            # Add to history
            self.query_history.append({
                "question": question,
                "timestamp": datetime.now().isoformat()
            })
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error processing query: {str(e)}")
            return {
                "question": question,
                "answer": "‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶ï‡¶∞‡¶£‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§",
                "source_documents": [],
                "num_sources": 0,
                "error": str(e)
            }
    
    def get_relevant_docs(self, query: str, k: int = 5) -> List[Document]:
        """Get relevant documents for a query"""
        try:
            docs = self.retriever.invoke(query)
            return docs[:k]
        except Exception as e:
            print(f"‚ùå Error retrieving documents: {str(e)}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics"""
        return {
            "total_queries": len(self.query_history),
            "total_documents": len(documents),
            "total_chunks": len(text_chunks),
            "index_stats": index.describe_index_stats()
        }

# Initialize the RAG system
bangla_rag = BanglaRAG(rag_chain, retriever)





#Checking Results

print("‚úÖ Advanced Bangla RAG System ready!")

# ================================================================
# STEP 15: Interactive Testing Interface
# ================================================================
print("\nüß™ System Testing & Demo Interface")
print("=" * 60)

# Display system stats
stats = bangla_rag.get_stats()
print(f"\nüìä System Statistics:")
print(f"   üìö Total Documents: {stats['total_documents']}")
print(f"   üìÑ Total Chunks: {stats['total_chunks']}")
print(f"   üîç Vector Index: {stats['index_stats']}")

# Sample test queries
sample_queries = [
    "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶∞ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨ ‡¶ï‡ßÄ?",
    "‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶¨‡¶≤‡ßÅ‡¶®‡•§",
    "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶õ‡¶®‡ßç‡¶¶‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∞‡¶≠‡ßá‡¶¶ ‡¶ï‡ßÄ ‡¶ï‡ßÄ?",
    "‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶∏‡¶Ç‡¶ó‡ßÄ‡¶§‡ßá‡¶∞ ‡¶ï‡¶¨‡¶ø ‡¶ï‡ßá?",
    "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶∞‡¶£‡ßá ‡¶ï‡¶Ø‡¶º‡¶ü‡¶ø ‡¶™‡¶¶ ‡¶Ü‡¶õ‡ßá?"
]

print(f"\nüìã Sample Test Queries:")
for i, query in enumerate(sample_queries, 1):
    print(f"   {i}. {query}")

# Interactive query function
def ask_question(question: str = None):
    """Interactive question asking function"""
    if not question:
        question = input("\nüí≠ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶® (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º): ").strip()
    
    if not question:
        print("‚ùå Please enter a valid question")
        return
    
    print("\n" + "="*50)
    print(f"‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®: {question}")
    print("="*50)
    
    # Get answer
    result = bangla_rag.query(question)
    
    # Display answer
    print(f"\nüìù ‡¶â‡¶§‡ßç‡¶§‡¶∞:")
    print("-" * 30)
    print(result['answer'])
    print("-" * 30)
    
    # Show sources if available
    if result.get('source_documents'):
        print(f"\nüìö ‡¶§‡¶•‡ßç‡¶Ø‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ({result['num_sources']}‡¶ü‡¶ø ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü):")
        for i, Updated_doc in enumerate(result['source_documents'][:3], 1):
            metadata = Updated_doc.metadata
            print(f"   {i}. {metadata.get('source', 'Unknown source')}")
            print(f"      {Updated_doc.page_content[:100]}...")
            print("-" * 30)

# ================================================================
# STEP 16: Interactive Testing Interface
# ================================================================
print("\nüß™ System Testing & Demo Interface")