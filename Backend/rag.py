# ================================================================
# COMPLETE BANGLA EDUCATIONAL RAG SYSTEM
# World-Class AI Engineering Implementation
# ================================================================

print("ЁЯЪА Initializing Bangla Educational RAG System...")
print("=" * 60)

# ================================================================
# STEP 1: Import Required Libraries
# ================================================================
print("\nЁЯУЪ Importing required libraries...")

import os
import sys
import warnings
import glob
import json
from pathlib import Path
from datetime import datetime
warnings.filterwarnings('ignore')
from dotenv import load_dotenv
# Core libraries
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from sentence_transformers import SentenceTransformer

# Text processing
import re
import unicodedata
from collections import defaultdict

# LangChain components
from langchain.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.embeddings.base import Embeddings
from langchain_groq import ChatGroq
from langchain.schema import Document

# Vector database
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore

print("тЬЕ All libraries imported successfully!")

# ================================================================
# STEP 2: Configuration and Constants
# ================================================================
print("\nтЪЩя╕П Setting up configuration...")

class Config:
    """Configuration class for the RAG system"""
    
    # Paths
    DOC_FOLDER = "doc"  # Your document folder
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    
    # Model settings
    EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
    LLM_MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"  # Better model for Bangla
    TEMPERATURE = 0.1
    MAX_TOKENS = 2048
    
    # Retrieval settings
    TOP_K_DOCS = 5
    
    # Pinecone settings
    INDEX_NAME = "rag"
    DIMENSION = 1024  #embedder model dimension
    METRIC = "cosine"
    
    # Language settings
    PRIMARY_LANGUAGE = "bangla"
    SECONDARY_LANGUAGE = "english"

config = Config()

# ================================================================
# STEP 3: API Keys Setup
# ================================================================
print("\nЁЯФС Setting up API keys...")

load_dotenv()

# Get API key from environment variable
groq_api_key = os.getenv("GROQ_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")

# Set environment variables

print("тЬЕ API keys configured!")

# ================================================================
# STEP 4: Bangla Text Processing Utilities
# ================================================================
print("\nЁЯФд Setting up Bangla text processing utilities...")

class BanglaTextProcessor:
    """Advanced Bangla text processing utilities"""
    
    @staticmethod
    def clean_bangla_text(text: str) -> str:
        """Clean and normalize Bangla text"""
        if not text:
            return ""
        
        # Normalize Unicode
        text = unicodedata.normalize('NFC', text)
        
        # Remove extra whitespaces
        text = re.sub(r'\s+', ' ', text)
        
        # Remove unwanted characters but keep Bangla punctuation
        text = re.sub(r'[^\u0980-\u09FF\u0020-\u007E\s\.\,\!\?\;\:\'\"\(\)\-]', '', text)
        
        # Clean up punctuation spacing
        text = re.sub(r'\s*([ред,!?;:])\s*', r'\1 ', text)
        
        return text.strip()
    
    @staticmethod
    def extract_metadata_from_filename(filename: str) -> Dict[str, str]:
        """Extract metadata from filename patterns"""
        filename = Path(filename).stem
        
        # Common patterns for educational files (optional)
        patterns = {
            'subject': r'(ржмрж╛ржВрж▓рж╛|ржЗржВрж░рзЗржЬрж┐|ржЧржгрж┐ржд|ржмрж┐ржЬрзНржЮрж╛ржи|рж╕ржорж╛ржЬ|ржЗрждрж┐рж╣рж╛рж╕|ржнрзВржЧрзЛрж▓)',
            'class': r'(class|рж╢рзНрж░рзЗржгрж┐)[\s\-_]*(\d+|ржПржХ|ржжрзБржЗ|рждрж┐ржи|ржЪрж╛рж░|ржкрж╛ржБржЪ|ржЫржпрж╝|рж╕рж╛ржд|ржЖржЯ|ржиржпрж╝|ржжрж╢)',
            'chapter': r'(chapter|ржЕржзрзНржпрж╛ржпрж╝)[\s\-_]*(\d+)',
            'book': r'(book|ржмржЗ)[\s\-_]*(.+)',
        }
        
        metadata = {}
        for key, pattern in patterns.items():
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                if key in ['class', 'chapter']:
                    metadata[key] = match.group(2)
                else:
                    metadata[key] = match.group(1)
        
        metadata['filename'] = filename
        return metadata
    
    @staticmethod
    def detect_content_type(text: str) -> str:
        """Detect the type of educational content"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['ржХржмрж┐рждрж╛', 'poem', 'ржЫржирзНржж', 'рж░рж╛ржЗржо']):
            return 'poetry'
        elif any(word in text_lower for word in ['ржЧрж▓рзНржк', 'story', 'ржЙржкржирзНржпрж╛рж╕', 'novel']):
            return 'literature'
        elif any(word in text_lower for word in ['ржмрзНржпрж╛ржХрж░ржг', 'grammar', 'ржмрж╛ржирж╛ржи', 'spelling']):
            return 'grammar'
        elif any(word in text_lower for word in ['ржЗрждрж┐рж╣рж╛рж╕', 'history', 'historical']):
            return 'history'
        elif any(word in text_lower for word in ['ржмрж┐ржЬрзНржЮрж╛ржи', 'science', 'scientific']):
            return 'science'
        elif any(word in text_lower for word in ['ржЧржгрж┐ржд', 'math', 'mathematics']):
            return 'mathematics'
        else:
            return 'general'

text_processor = BanglaTextProcessor()

# ================================================================
# STEP 5: Enhanced Multilingual Embeddings
# ================================================================
print("\nЁЯза Setting up Enhanced Multilingual Embeddings...")

class EnhancedBanglaEmbeddings(Embeddings):
    """
    Enhanced embedding class optimized for Bangla educational content
    """
    
    def __init__(self, model_name: str = config.EMBEDDING_MODEL):
        print(f"Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)
        
        # Optimize for multilingual performance
        self.model.max_seq_length = 512
        
        print("тЬЕ Enhanced Bangla embedding model loaded successfully!")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed documents with optimized prefixes for educational content"""
        # Clean texts first
        cleaned_texts = [text_processor.clean_bangla_text(text) for text in texts]
        
        # Add educational passage prefix for better retrieval
        prefixed_texts = [f"passage: {text}" for text in cleaned_texts]
        
        # Generate embeddings with normalization
        embeddings = self.model.encode(
            prefixed_texts, 
            normalize_embeddings=True,
            batch_size=32,
            show_progress_bar=True
        )
        return embeddings.tolist()
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query with educational context"""
        # Clean query text
        cleaned_text = text_processor.clean_bangla_text(text)
        
        # Add educational query prefix
        prefixed_text = f"query: {cleaned_text}"
        
        # Generate embedding
        embedding = self.model.encode(
            prefixed_text, 
            normalize_embeddings=True
        )
        return embedding.tolist()

# Initialize embeddings
embeddings = EnhancedBanglaEmbeddings()

# ================================================================
# STEP 6: Document Loader and Processor
# ================================================================
print("\nЁЯУД Setting up document loader and processor...")

class BanglaDocumentLoader:
    """Advanced document loader for Bangla educational content"""
    
    def __init__(self, doc_folder: str = config.DOC_FOLDER):
        self.doc_folder = doc_folder
        self.supported_formats = ['.txt', '.md']
    
    def load_documents(self) -> List[Document]:
        """Load all documents from the doc folder with enhanced metadata"""
        documents = []
        
        if not os.path.exists(self.doc_folder):
            print(f"тЪая╕П Creating doc folder: {self.doc_folder}")
            os.makedirs(self.doc_folder)
            print("ЁЯУЭ Please add your Bangla educational txt files to the 'doc' folder")
            return documents
        
        # Get all text files
        file_patterns = [f"**/*{ext}" for ext in self.supported_formats]
        all_files = []
        
        for pattern in file_patterns:
            all_files.extend(glob.glob(os.path.join(self.doc_folder, pattern), recursive=True))
        
        print(f"ЁЯУЪ Found {len(all_files)} files to process")
        
        for file_path in all_files:
            try:
                # Load document with proper encoding
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if not content.strip():
                    continue
                
                # Clean content
                cleaned_content = text_processor.clean_bangla_text(content)
                
                # Extract metadata
                base_metadata = text_processor.extract_metadata_from_filename(file_path)
                
                # Enhanced metadata
                metadata = {
                    **base_metadata,
                    'source': file_path,
                    'file_size': len(content),
                    'content_type': text_processor.detect_content_type(content),
                    'language': 'bangla' if self._is_bangla_content(content) else 'mixed',
                    'processed_at': datetime.now().isoformat(),
                    'char_count': len(cleaned_content),
                    'word_count': len(cleaned_content.split())
                }
                
                # Create document
                doc = Document(
                    page_content=cleaned_content,
                    metadata=metadata
                )
                documents.append(doc)
                
                print(f"тЬЕ Processed: {os.path.basename(file_path)}")
                
            except Exception as e:
                print(f"тЭМ Error processing {file_path}: {str(e)}")
                continue
        
        print(f"тЬЕ Successfully loaded {len(documents)} documents")
        return documents
    
    def _is_bangla_content(self, text: str) -> bool:
        """Check if text is primarily in Bangla"""
        bangla_chars = len(re.findall(r'[\u0980-\u09FF]', text))
        total_chars = len(re.findall(r'[^\s\d\W]', text))
        return bangla_chars > (total_chars * 0.6) if total_chars > 0 else False

# ================================================================
# STEP 7: Advanced Text Splitter
# ================================================================
print("\nтЬВя╕П Setting up advanced text splitter...")

class BanglaTextSplitter(RecursiveCharacterTextSplitter):
    """Custom text splitter optimized for Bangla educational content"""
    
    def __init__(self, chunk_size: int = config.CHUNK_SIZE, chunk_overlap: int = config.CHUNK_OVERLAP):
        # Bangla-specific separators
        separators = [
            "\n\n",  # Paragraph breaks
            "ред\n",   # Bangla sentence end
            "ред",     # Bangla sentence end
            "\n",    # Line breaks
            "редред",    # Double danda
            "редредред",   # Triple danda
            ". ",    # English sentence end
            "! ",    # Exclamation
            "? ",    # Question
            "; ",    # Semicolon
            ", ",    # Comma
            " ",     # Space
            ""       # Character level
        ]
        
        super().__init__(
            separators=separators,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False
        )

# ================================================================
# STEP 8: Load and Process Documents
# ================================================================
print("\nЁЯУЦ Loading and processing documents...")

# Initialize document loader
doc_loader = BanglaDocumentLoader()

# Load documents
documents = doc_loader.load_documents()

if not documents:
    print("тЪая╕П No documents found. Creating sample documents for demonstration...")
    
    # Create sample educational content
    sample_docs = [
        Document(
            page_content="""
            ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржУ рж╕рж╛рж╣рж┐рждрзНржп

            ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржжржХрзНрж╖рж┐ржг ржПрж╢рж┐ржпрж╝рж╛рж░ ржПржХржЯрж┐ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржнрж╛рж╖рж╛ред ржПржЯрж┐ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж░рж╛рж╖рзНржЯрзНрж░ржнрж╛рж╖рж╛ ржПржмржВ ржнрж╛рж░рждрзЗрж░ ржкрж╢рзНржЪрж┐ржоржмржЩрзНржЧ рж░рж╛ржЬрзНржпрзЗрж░ ржжрж╛ржкрзНрждрж░рж┐ржХ ржнрж╛рж╖рж╛ред

            ржмрж╛ржВрж▓рж╛ рж╕рж╛рж╣рж┐рждрзНржпрзЗрж░ ржЗрждрж┐рж╣рж╛рж╕ ржЕрждрзНржпржирзНржд рж╕ржорзГржжрзНржзред рж░ржмрзАржирзНржжрзНрж░ржирж╛рже ржарж╛ржХрзБрж░, ржиржЬрж░рзБрж▓ ржЗрж╕рж▓рж╛ржо, ржмржЩрзНржХрж┐ржоржЪржирзНржжрзНрж░ ржЪржЯрзНржЯрзЛржкрж╛ржзрзНржпрж╛ржпрж╝ ржкрзНрж░ржорзБржЦ ржорж╣рж╛ржи рж╕рж╛рж╣рж┐рждрзНржпрж┐ржХржжрзЗрж░ ржЕржмржжрж╛ржирзЗ ржмрж╛ржВрж▓рж╛ рж╕рж╛рж╣рж┐рждрзНржп ржмрж┐рж╢рзНржмржорж╛ржирзЗрж░ред

            ржмрзНржпрж╛ржХрж░ржг:
            ржмрж╛ржВрж▓рж╛ ржмрзНржпрж╛ржХрж░ржгрзЗ ржмрж┐рж╢рзЗрж╖рзНржп, рж╕рж░рзНржмржирж╛ржо, ржмрж┐рж╢рзЗрж╖ржг, ржХрзНрж░рж┐ржпрж╝рж╛, ржЕржмрзНржпржпрж╝ - ржПржЗ ржкрж╛ржБржЪржЯрж┐ ржорзВрж▓ ржкржж рж░ржпрж╝рзЗржЫрзЗред
            """,
            metadata={
                'subject': 'ржмрж╛ржВрж▓рж╛',
                'content_type': 'literature',
                'language': 'bangla',
                'source': 'sample_bangla_literature.txt'
            }
        ),
        Document(
            page_content="""
            ржЫржирзНржж ржУ ржХржмрж┐рждрж╛

            ржЫржирзНржж рж╣рж▓рзЛ ржХржмрж┐рждрж╛рж░ ржкрзНрж░рж╛ржгред ржмрж╛ржВрж▓рж╛ ржЫржирзНржжрзЗрж░ рждрж┐ржиржЯрж┐ ржкрзНрж░ржзрж╛ржи ржнрж╛ржЧ:
            рзз. ржЕржХрзНрж╖рж░ржмрзГрждрзНржд ржЫржирзНржж
            рзи. ржорж╛рждрзНрж░рж╛ржмрзГрждрзНржд ржЫржирзНржж  
            рзй. рж╕рзНржмрж░ржмрзГрждрзНржд ржЫржирзНржж

            рж░ржмрзАржирзНржжрзНрж░ржирж╛ржерзЗрж░ ржмрж┐ржЦрзНржпрж╛ржд ржХржмрж┐рждрж╛:
            "ржЖржорж╛рж░ рж╕рзЛржирж╛рж░ ржмрж╛ржВрж▓рж╛, ржЖржорж┐ рждрзЛржорж╛ржпрж╝ ржнрж╛рж▓рзЛржмрж╛рж╕рж┐ред
            ржЪрж┐рж░ржжрж┐ржи рждрзЛржорж╛рж░ ржЖржХрж╛рж╢, рждрзЛржорж╛рж░ ржмрж╛рждрж╛рж╕, ржЖржорж╛рж░ ржкрзНрж░рж╛ржгрзЗ ржмрж╛ржЬрж╛ржпрж╝ ржмрж╛ржБрж╢рж┐ред"

            ржПржЯрж┐ ржЖржорж╛ржжрзЗрж░ ржЬрж╛рждрзАржпрж╝ рж╕ржВржЧрзАрждред
            """,
            metadata={
                'subject': 'ржмрж╛ржВрж▓рж╛',
                'content_type': 'poetry',
                'language': 'bangla',
                'source': 'sample_poetry.txt'
            }
        )
    ]
    documents = sample_docs

# Initialize text splitter
text_splitter = BanglaTextSplitter()

# Split documents into chunks
print(f"ЁЯУЭ Splitting {len(documents)} documents into chunks...")
text_chunks = text_splitter.split_documents(documents)

print(f"тЬЕ Created {len(text_chunks)} text chunks")

# Display sample chunk info
if text_chunks:
    sample_chunk = text_chunks[0]
    print(f"\nЁЯУЛ Sample chunk preview:")
    print(f"Content length: {len(sample_chunk.page_content)} characters")
    print(f"Metadata: {sample_chunk.metadata}")
    print(f"Content preview: {sample_chunk.page_content[:200]}...")

# ================================================================
# STEP 9: Initialize GROQ LLM
# ================================================================
print("\nЁЯдЦ Setting up Enhanced GROQ LLM...")

# Initialize the GROQ LLM with better model for Bangla
llm = ChatGroq(
    model=config.LLM_MODEL,
    temperature=config.TEMPERATURE,
    max_tokens=config.MAX_TOKENS,
    timeout=60,
    max_retries=3
)

print("тЬЕ Enhanced GROQ LLM initialized successfully!")

# ================================================================
# STEP 10: Setup Pinecone Vector Database
# ================================================================
print("\nЁЯЧДя╕П Setting up Enhanced Pinecone Vector Database...")

# Initialize Pinecone client
pc = Pinecone(api_key=pinecone_api_key)

# Check if index exists, create if not
existing_indexes = pc.list_indexes().names()
if config.INDEX_NAME not in existing_indexes:
    print(f"Creating new Pinecone index: {config.INDEX_NAME}")
    pc.create_index(
        name=config.INDEX_NAME,
        dimension=config.DIMENSION,
        metric=config.METRIC,
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )
    print("тЬЕ New Pinecone index created!")
else:
    print(f"тЬЕ Using existing Pinecone index: {config.INDEX_NAME}")

# Get index reference
index = pc.Index(config.INDEX_NAME)

# ================================================================
# STEP 11: Create Vector Store and Retriever
# ================================================================
print("\nЁЯФН Creating enhanced vector store and retriever...")

# Check current vectors in index
stats = index.describe_index_stats()
total_vectors = stats.get('total_vector_count', 0)
print(f"Current vectors in index: {total_vectors}")

# Create or connect to vectorstore
vectorstore = PineconeVectorStore.from_documents(
    documents=text_chunks,
    index_name=config.INDEX_NAME,
    embedding=embeddings
) if total_vectors == 0 else PineconeVectorStore(
    index_name=config.INDEX_NAME,
    embedding=embeddings
)

# Setup enhanced retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": config.TOP_K_DOCS}
)

print("тЬЕ Enhanced vector store and retriever created!")

# ================================================================
# STEP 12: Bangla Educational Prompt Template
# ================================================================
print("\nЁЯУЭ Setting up Bangla educational prompt template...")

# Enhanced system prompt for Bangla education
system_prompt = """
ржЖржкржирж┐ ржПржХржЬржи ржЕржнрж┐ржЬрзНржЮ ржПржмржВ ржжржХрзНрж╖ ржмрж╛ржВрж▓рж╛ рж╢рж┐ржХрзНрж╖ржХред ржЖржкржирж╛рж░ ржжрж╛ржпрж╝рж┐рждрзНржм рж╣рж▓рзЛ рж╢рж┐ржХрзНрж╖рж╛рж░рзНржерзАржжрзЗрж░ ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛, рж╕рж╛рж╣рж┐рждрзНржп ржПржмржВ рж╕ржВрж╕рзНржХрзГрждрж┐ рж╕ржорзНржкрж░рзНржХрзЗ рж╕ржарж┐ржХ ржУ рж╕рж╣рж╛ржпрж╝ржХ рждржерзНржп ржкрзНрж░ржжрж╛ржи ржХрж░рж╛ред

## ржЖржкржирж╛рж░ ржмрж┐рж╢рзЗрж╖рждрзНржм:
- ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржУ ржмрзНржпрж╛ржХрж░ржг
- ржмрж╛ржВрж▓рж╛ рж╕рж╛рж╣рж┐рждрзНржп ржУ ржХржмрж┐рждрж╛
- ржмрж╛ржВрж▓рж╛ рж╕ржВрж╕рзНржХрзГрждрж┐ ржУ ржРрждрж┐рж╣рзНржп
- рж╢рж┐ржХрзНрж╖рж╛ржорзВрж▓ржХ ржмрж┐рж╖ржпрж╝рж╛ржмрж▓рзА

## ржЙрждрзНрждрж░ ржкрзНрж░ржжрж╛ржирзЗрж░ ржирж┐ржпрж╝ржорж╛ржмрж▓рзА:

### рзз. ржнрж╛рж╖рж╛ ржУ рж╕рзНржЯрж╛ржЗрж▓:
- рж╕рж░рзНржмржжрж╛ ржмрж╛ржВрж▓рж╛ржпрж╝ ржЙрждрзНрждрж░ ржжрж┐ржи
- рж╕рж╣ржЬ, рж╕рж░рж▓ ржУ ржмрзЛржзржЧржорзНржп ржнрж╛рж╖рж╛ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
- рж╢рж┐ржХрзНрж╖рж╛рж░рзНржерзАржжрзЗрж░ ржЙржкржпрзЛржЧрзА ржХрж░рзЗ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рзБржи
- ржкрзНрж░ржпрж╝рзЛржЬржирзЗ ржЙржжрж╛рж╣рж░ржг ржжрж┐ржи

### рзи. рждржерзНржпрзЗрж░ ржирж┐рж░рзНржнрж░ржпрзЛржЧрзНржпрждрж╛:
- рж╢рзБржзрзБржорж╛рждрзНрж░ ржкрзНрж░ржжрждрзНржд context ржерзЗржХрзЗ рждржерзНржп ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
- ржЕржирзБржорж╛ржиржнрж┐рждрзНрждрж┐ржХ рждржерзНржп ржкрзНрж░ржжрж╛ржи ржХрж░ржмрзЗржи ржирж╛
- рж╕рзВрждрзНрж░ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рзБржи ржпржЦржи ржкрзНрж░ржпрж╝рзЛржЬржи

### рзй. рж╢рж┐ржХрзНрж╖рж╛ржорзВрж▓ржХ ржкржжрзНржзрждрж┐:
- ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рзБржи
- ржорзВрж▓ ржмрж┐рж╖ржпрж╝ржЯрж┐ ржкрзНрж░ржержорзЗ ржмрж▓рзБржи
- рждрж╛рж░ржкрж░ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржмрзНржпрж╛ржЦрзНржпрж╛ ржжрж┐ржи
- ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЙржжрж╛рж╣рж░ржг рж╕рж╣ ржЙржкрж╕рзНржерж╛ржкржи ржХрж░рзБржи

### рзк. ржЙрждрзНрждрж░рзЗрж░ ржХрж╛ржарж╛ржорзЛ:
- рж╕рзНржкрж╖рзНржЯ ржУ рж╕ржВржЧржарж┐ржд ржЙрждрзНрждрж░ ржжрж┐ржи
- ржкрзНрж░ржпрж╝рзЛржЬржирзЗ ржкржпрж╝рзЗржирзНржЯ ржЖржХрж╛рж░рзЗ рж▓рж┐ржЦрзБржи
- ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржмрж┐рж╖ржпрж╝ рждрзБрж▓рзЗ ржзрж░рзБржи

### рзл. ржмрж┐рж╢рзЗрж╖ ржХрзНрж╖рзЗрждрзНрж░рзЗ:
- ржХржмрж┐рждрж╛ ржмрж╛ рж╕рж╛рж╣рж┐рждрзНржпрзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржнрж╛ржм ржУ ржЕрж░рзНрже ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рзБржи
- ржмрзНржпрж╛ржХрж░ржгрзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржирж┐ржпрж╝ржо ржУ ржЙржжрж╛рж╣рж░ржг ржжрж┐ржи
- ржЗрждрж┐рж╣рж╛рж╕ ржмрж╛ рж╕ржВрж╕рзНржХрзГрждрж┐рж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржкрзНрж░рзЗржХрзНрж╖рж╛ржкржЯ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рзБржи

### рзм. ржЕржЬрж╛ржирж╛ рждржерзНржпрзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ:
ржпржжрж┐ ржкрзНрж░ржжрждрзНржд рждржерзНржпрзЗ ржЙрждрзНрждрж░ ржирж╛ ржерж╛ржХрзЗ, рждрж╛рж╣рж▓рзЗ ржмрж▓рзБржи:
"ржжрзБржГржЦрж┐ржд, ржкрзНрж░ржжрждрзНржд рждржерзНржпрзЗ ржПржЗ ржмрж┐рж╖ржпрж╝рзЗ ржпржерзЗрж╖рзНржЯ рждржерзНржп ржирзЗржЗред ржЖрж░ржУ ржирж┐рж░рзНржнрж░ржпрзЛржЧрзНржп рж╕рзВрждрзНрж░ ржмрж╛ ржмрж┐рж╢рзЗрж╖ржЬрзНржЮрзЗрж░ рж╕рж╛рж╣рж╛ржпрзНржп ржирж┐ржиред"

ржкрзНрж░рж╕ржЩрзНржЧ (Context): {context}

ржЖржкржирж╛рж░ ржЙрждрзНрждрж░ ржЕржмрж╢рзНржпржЗ:
тЬУ ржмрж╛ржВрж▓рж╛ржпрж╝ рж╣рждрзЗ рж╣ржмрзЗ
тЬУ рж╢рж┐ржХрзНрж╖рж╛ржорзВрж▓ржХ рж╣рждрзЗ рж╣ржмрзЗ  
тЬУ рж╕ржарж┐ржХ ржУ ржирж┐рж░рзНржнрж░ржпрзЛржЧрзНржп рж╣рждрзЗ рж╣ржмрзЗ
тЬУ рж╕рж╣ржЬржмрзЛржзрзНржп рж╣рждрзЗ рж╣ржмрзЗ
"""

# Create enhanced prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

print("тЬЕ Bangla educational prompt template configured!")

# ================================================================
# STEP 13: Create Enhanced RAG Chain
# ================================================================
print("\nЁЯФЧ Creating Enhanced RAG Chain...")

# Create document chain
question_answer_chain = create_stuff_documents_chain(llm, prompt)

# Create retrieval chain
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

print("тЬЕ Enhanced RAG chain created successfully!")

# ================================================================
# STEP 14: Advanced Query System
# ================================================================
print("\nЁЯОп Setting up Advanced Query System...")

class BanglaRAG:
    """Advanced Bangla Educational RAG System"""
    
    def __init__(self, rag_chain, retriever):
        self.rag_chain = rag_chain
        self.retriever = retriever
        self.query_history = []
    
    def query(self, question: str, show_sources: bool = True) -> Dict[str, Any]:
        """
        Process educational query with enhanced features
        """
        print(f"\nЁЯФН ржкрзНрж░рж╢рзНржи ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ржХрж░ржг: {question}")
        
        # Clean and process question
        cleaned_question = text_processor.clean_bangla_text(question)
        
        # Get response from RAG chain
        try:
            response = self.rag_chain.invoke({"input": cleaned_question})
            
            # Extract information
            answer = response.get("answer", "")
            source_docs = response.get("context", [])
            
            # Prepare result
            result = {
                "question": question,
                "answer": answer,
                "source_documents": source_docs,
                "num_sources": len(source_docs),
                "timestamp": datetime.now().isoformat()
            }
            
            # Add to history
            self.query_history.append({
                "question": question,
                "timestamp": datetime.now().isoformat()
            })
            
            return result
            
        except Exception as e:
            print(f"тЭМ Error processing query: {str(e)}")
            return {
                "question": question,
                "answer": "ржжрзБржГржЦрж┐ржд, ржкрзНрж░рж╢рзНржи ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ржХрж░ржгрзЗ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝рзЗржЫрзЗред ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржЖржмрж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржиред",
                "source_documents": [],
                "num_sources": 0,
                "error": str(e)
            }
    
    def get_relevant_docs(self, query: str, k: int = 5) -> List[Document]:
        """Get relevant documents for a query"""
        try:
            docs = self.retriever.invoke(query)
            return docs[:k]
        except Exception as e:
            print(f"тЭМ Error retrieving documents: {str(e)}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """Get system statistics"""
        return {
            "total_queries": len(self.query_history),
            "total_documents": len(documents),
            "total_chunks": len(text_chunks),
            "index_stats": index.describe_index_stats()
        }

# Initialize the RAG system
bangla_rag = BanglaRAG(rag_chain, retriever)





#Checking Results

print("тЬЕ Advanced Bangla RAG System ready!")

# ================================================================
# STEP 15: Interactive Testing Interface
# ================================================================
print("\nЁЯзк System Testing & Demo Interface")
print("=" * 60)

# Display system stats
stats = bangla_rag.get_stats()
print(f"\nЁЯУК System Statistics:")
print(f"   ЁЯУЪ Total Documents: {stats['total_documents']}")
print(f"   ЁЯУД Total Chunks: {stats['total_chunks']}")
print(f"   ЁЯФН Vector Index: {stats['index_stats']}")

# Sample test queries
sample_queries = [
    "ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛рж░ ржЧрзБрж░рзБрждрзНржм ржХрзА?",
    "рж░ржмрзАржирзНржжрзНрж░ржирж╛рже ржарж╛ржХрзБрж░ рж╕ржорзНржкрж░рзНржХрзЗ ржмрж▓рзБржиред",
    "ржмрж╛ржВрж▓рж╛ ржЫржирзНржжрзЗрж░ ржкрзНрж░ржХрж╛рж░ржнрзЗржж ржХрзА ржХрзА?",
    "ржЬрж╛рждрзАржпрж╝ рж╕ржВржЧрзАрждрзЗрж░ ржХржмрж┐ ржХрзЗ?",
    "ржмрж╛ржВрж▓рж╛ ржмрзНржпрж╛ржХрж░ржгрзЗ ржХржпрж╝ржЯрж┐ ржкржж ржЖржЫрзЗ?"
]

print(f"\nЁЯУЛ Sample Test Queries:")
for i, query in enumerate(sample_queries, 1):
    print(f"   {i}. {query}")

# Interactive query function
def ask_question(question: str = None):
    """Interactive question asking function"""
    if not question:
        question = input("\nЁЯТн ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзБржи (ржмрж╛ржВрж▓рж╛ржпрж╝): ").strip()
    
    if not question:
        print("тЭМ Please enter a valid question")
        return
    
    print("\n" + "="*50)
    print(f"ржкрзНрж░рж╢рзНржи: {question}")
    print("="*50)
    
    # Get answer
    result = bangla_rag.query(question)
    
    # Display answer
    print(f"\nЁЯУЭ ржЙрждрзНрждрж░:")
    print("-" * 30)
    print(result['answer'])
    print("-" * 30)
    
    # Show sources if available
    if result.get('source_documents'):
        print(f"\nЁЯУЪ рждржерзНржпрж╕рзВрждрзНрж░ ({result['num_sources']}ржЯрж┐ ржбржХрзБржорзЗржирзНржЯ):")
        for i, Updated_doc in enumerate(result['source_documents'][:3], 1):
            metadata = Updated_doc.metadata
            print(f"   {i}. {metadata.get('source', 'Unknown source')}")
            print(f"      {Updated_doc.page_content[:100]}...")
            print("-" * 30)

# ================================================================
# STEP 16: Interactive Testing Interface
# ================================================================
print("\nЁЯзк System Testing & Demo Interface")